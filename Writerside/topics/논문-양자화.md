# 논문_양자화

양자화(Quantization) 은 딥러닝 모델에서 양자화를 사용하여 모델의 크기를 줄이고, 추론 속도를 높이며 메모리 사용량을 최적화하는 방법을 설명하는 내용을 다루고 있습니다. 간략하게 요약하면 다음과 같습니다:

1. 개요 (Abstract)
   * **양자화(Quantization)** 는 딥러닝 모델의 크기를 줄이고 추론 속도와 처리량을 향상시키는 기법입니다. 이 논문에서는 8비트 양자화 워크플로우를 소개하고, 부동소수점 모델과 비교하여 1% 이내의 정확도를 유지할 수 있음을 강조합니다).
2. 관련 연구 (Related Work)
   * 양자화의 두 가지 방법인 **Uniform Quantization(균일 양자화)** 와 **Non-Uniform Quantization(비균일 양자화)** 를 소개합니다. Non-Uniform Quantization은 복잡도가 높아 계산 효율성이 떨어질 수 있으므로, 대부분의 연구에서 Uniform Quantization을 사용합니다).
3. 양자화 기초 (Quantization Fundamentals)
   * **Affine Quantization(비대칭적 양자화)** 와 **Scale Quantization(대칭적 양자화)** 방식을 설명하며, 각각의 계산 방식과 적용 방법을 다룹니다.
   * 양자화는 부동소수점을 정수로 변환하고, 다시 원래의 값으로 복원하는 과정을 거치며, 이를 통해 계산 복잡도를 줄입니다).
4. Post Training Quantization (PTQ)
   * **PTQ(사전 훈련 후 양자화)** 는 학습이 완료된 모델에 대해 추가적인 학습 없이 양자화를 적용하는 방식입니다. 가중치와 활성화 값의 양자화가 적용되며, 성능 향상을 위해 다양한 보정 기법(Max, Entropy, Percentile)을 사용할 수 있습니다).
5. 정확도 복구 기법 (Techniques to Recover Accuracy)
   * Partial Quantization과 QAT(Quantization-Aware Training) 같은 방법으로 양자화로 인한 정확도 손실을 최소화할 수 있습니다.
   * QAT는 모델 훈련 중 양자화를 시뮬레이션하며, Fake Quantization을 통해 실제 양자화된 환경에서 모델을 학습합니다).
6. 권장 워크플로우 (Recommended Workflow)
   * 가중치와 활성화를 각각 적절한 방식으로 양자화하고, PTQ 또는 QAT를 사용하여 최적의 성능과 정확도를 유지하는 방법을 제시합니다).
7. 결론 (Conclusion)
   * 다양한 신경망 모델에 대해 양자화 기법을 평가한 결과, 대부분의 모델이 양자화 후에도 부동소수점 모델과 유사한 정확도를 유지할 수 있음을 확인하였습니다