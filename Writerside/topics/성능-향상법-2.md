# 성능 향상법(2)
1. Sampling and Quantization
   * 샘플링과 양자화는 신경망의 계산 효율성을 높이기 위한 중요한 기술입니다.
   * **양자화(Quantization)** 는 주어진 비트 수 내에서 수치 표현을 제한함으로써 연산 및 메모리 사용량을 줄이는 방법입니다.
   * Direct Quantization:
     * 이 기법은 고정된 비트폭 내에서 값을 양자화하는 방식으로, 수치적 오류와 양자화 노이즈가 발생할 수 있습니다.
   * Linear Quantization:
     * Floating-point 수치를 양자화할 때, 클리핑(clipping)을 사용해 최솟값과 최댓값을 설정해 표현 범위를 제한합니다

2. Lower Numerical Precision
   * 신경망 연산에서 32비트 부동소수점 연산을 사용하는 것은 일반적이지만, 모든 연산에 높은 수치적 정밀도가 필요한 것은 아닙니다.
   * Brain Floating Point (BFloat16):
     * FP32와 BFloat16 간의 변환이 쉬우며, FP32의 지수 범위를 유지하면서 작은 네트워크에서도 비슷한 학습 성능을 제공합니다​(4_Thinking_machine_ch6_…).
   * 16-bit Fixed-point:
     * 고정소수점 표현은 메모리 절약에 유리하며, 적절한 반올림 기법을 적용하면 단일 정밀도 부동소수점 연산과 비슷한 추론 성능을 낼 수 있습니다.
   * Dynamic Precision:
     * 고정 소수점에서 정수는 동일한 비트폭을 유지하면서 소수점의 비트폭을 동적으로 조절하여 추론 성능을 향상시킬 수 있습니다

3. Memory Requirement
   * 메모리 요구 사항은 수치 정밀도가 줄어들수록 감소하며, 이는 특히 미니 배치 크기가 큰 학습 및 추론 작업에서 효율적으로 작용합니다.
   * Effective Compression Ratio (ECR):
     * 네트워크 모델이 동일한 정확도를 유지하면서 압축된 메모리 비트 수와의 비율을 나타냅니다. 5비트 양자화가 ECR 면에서 가장 좋은 성능을 보입니다.
   * Memory Impact on Training & Inference:
     * 메모리 사용량을 줄이는 것은 특히 추론 단계에서 중요한데, 활성화 데이터는 총 데이터의 90% 이상을 차지할 수 있습니다

4. Side-effect on Inference Accuracy
   * 고정소수점 표현에서 추론 및 학습 정확도에 미치는 영향을 실험했습니다.
   * 16비트 고정소수점 표현은 학습에 적합하지 않으며, 이로 인해 혼합 정밀도 아키텍처의 필요성이 제기됩니다.
   * Precision & Accuracy Trade-off:
     * LeNet은 2/4비트 양자화에서도 성능이 우수한 반면, CIFAR-10 및 ImageNet CNN 모델은 8비트 양자화에서 더 나은 성능을 보였습니다.
   * AlexNet:
     * 4비트 미만의 양자화에서는 정확도가 급격히 감소하며, 특히 활성화 정밀도를 줄이면 정확도에 더 큰 영향을 미칩니다

5. Effect on Execution Performance
   * 바이너리화된 모델에서는 곱셈 연산이 XNOR 연산으로 대체되어 성능이 크게 향상됩니다.
   * 하지만 필터 크기와 채널 수가 커지면 성능 향상은 제한적일 수 있으며, 메모리 병목현상이 발생할 수 있습니다.
6. Effect on Reduction of Energy Consumption
   * 8비트 곱셈을 사용하면 에너지 소비가 최소 4배 감소하고, 대규모 행렬 연산에서는 18배까지 에너지 절약 효과가 나타납니다.
   * CIFAR-10 분류 문제에서 FP32 기반 모델과 양자화 모델 간의 에너지 소비 비교를 통해, 낮은 정밀도가 하드웨어 구현 비용을 줄이는 효과가 있다는 것이 입증되었습니다
7. Edge-cutting and Clipping
   * Edge-cutting은 하위 비트 필드의 일부를 잘라내거나 값을 0으로 설정하여 데이터를 축소하는 방법입니다. 이는 특히 Bfloat16 또는 고정소수점 수에서 유용합니다.
   * Clipping은 입력 값의 최댓값과 최솟값을 설정해 포화 상태를 만드는 방법으로, ReLU6와 같은 활성화 함수에서 자주 사용됩니다.
   * 이러한 방법은 **가중치 공유(weight-sharing)**와 결합해 메모리 액세스를 줄이고, 에너지 소비를 낮출 수 있습니다
8. Optimization
   * 최적화 기법은 주로 메모리 액세스를 줄이고, 데이터 흐름을 최적화하여 전반적인 하드웨어 성능을 향상시키는 데 사용됩니다
9. Summary
   * 이 장에서는 수치적 양자화, 고정 소수점 정밀도, 메모리 요구 사항, 추론 정확도에 미치는 영향 등 다양한 성능 향상 기법을 다루었습니다.
   * 성능을 높이면서도 에너지 소비를 줄이기 위한 다양한 방법이 제안되었으며, 특히 혼합 정밀도, 바이너리화, edge-cutting 등이 중요한 기법으로 강조되었습니다