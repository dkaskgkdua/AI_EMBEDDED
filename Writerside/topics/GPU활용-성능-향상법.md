# GPU활용 성능 향상법
GPU(그래픽 처리 장치)를 사용한 딥러닝 성능 향상 방법을 다루고 있습니다

1. GPU를 활용한 딥러닝 응용
   * GPU와 딥러닝: GPU는 대량의 병렬 처리를 통해 복잡한 연산을 빠르게 처리할 수 있어 딥러닝 모델의 학습에 매우 유리합니다.
   * 적용 사례:
     * IoT, 스마트 시티, 헬스케어: GPU는 AI 모델 학습을 중앙 서버에서 수행한 후, 엣지 장치나 임베디드 시스템에 배포하여 활용됩니다. 예를 들어, 스마트 시티의 카메라나 헬스케어 기기에서 GPU 기반 모델을 사용해 데이터를 처리합니다.
     * 자율 로봇 및 자율 주행: 자율 로봇이나 차량에서도 GPU가 사용되어, 실시간 경로 탐색 및 시각적 인식을 가속화합니다. 학습은 중앙 서버에서 수행되지만, 추론은 차량이나 로봇 내의 임베디드 보드에서 GPU를 활용하여 이루어집니다
2. GPU 개요
   * GPU 아키텍처: GPU는 수천 개의 코어로 구성된 병렬 처리 장치로, 특히 행렬 연산과 같은 대량의 데이터를 동시에 처리하는 데 매우 적합합니다. 현대 GPU, 예를 들어 NVIDIA A100은 여러 **스트리밍 멀티프로세서(SM)** 를 포함하고 있으며, 각 SM은 다수의 CUDA 코어를 포함하고 있습니다.
   * 병렬 처리: GPU는 SIMT(Single Instruction, Multiple Threads) 모델을 사용하여 다수의 쓰레드를 동시에 처리합니다. 이를 통해 CPU보다 훨씬 더 많은 연산을 빠르게 처리할 수 있습니다.
   * 메모리 계층: GPU는 빠른 메모리 계층 구조(예: 공유 메모리, L1 캐시)를 사용하여 연산 중 지연 시간을 최소화합니다
3. 딥러닝 가속을 위한 GPU 하드웨어 측면
   * Tensor Cores: NVIDIA의 Volta 아키텍처에서 도입된 Tensor Cores는 딥러닝 연산, 특히 행렬 연산을 위한 특수 하드웨어입니다. 이 코어는 혼합 정밀도 연산(FP16 입력, FP32 누적)을 지원하여 행렬 곱셈을 가속화합니다.
   * 메모리 대역폭: GPU는 **HBM(High-Bandwidth Memory)**과 같은 고속 메모리를 사용하여 대량의 데이터를 신속하게 처리합니다. 예를 들어, Tesla P100 및 A100 GPU는 HBM2를 사용하여 1.5 TB/s 이상의 메모리 대역폭을 제공합니다
4. 딥러닝 가속을 위한 GPU 소프트웨어
   * CUDA: NVIDIA의 CUDA는 병렬 컴퓨팅 플랫폼 및 프로그래밍 모델입니다. CUDA API를 사용하여 GPU 메모리를 할당하고 데이터를 GPU로 전송할 수 있습니다.
   * cuDNN: cuDNN은 딥러닝에서 사용되는 주요 연산(예: 컨볼루션, 풀링)을 가속화하는 라이브러리입니다. 특히 Tensor Cores를 지원하여 GPU에서 딥러닝 모델 학습과 추론을 빠르게 수행할 수 있습니다.
   * TensorRT: TensorRT는 GPU에서 딥러닝 모델을 최적화하고 배포하기 위한 소프트웨어 툴킷입니다. TensorRT는 정밀도 조정 및 커널 자동 조정을 통해 GPU에서 추론 속도를 크게 향상시킵니다
5. GPU에서 딥러닝 모델 최적화를 위한 고급 기법
   * Pruning: Pruning(가지치기) 기법은 불필요한 뉴런이나 가중치를 제거하여 모델의 크기를 줄이고 연산 효율을 높입니다. 벡터 수준의 가지치기 및 희소성 인코딩을 통해 GPU에서 이러한 가지치기 모델을 더욱 빠르게 처리할 수 있습니다.
   * 데이터 레이아웃 최적화: CNN 모델의 경우 특정 레이아웃(예: CHWN)을 사용하여 메모리 접근 패턴을 최적화할 수 있으며, 이를 통해 GPU 메모리 사용을 최소화하고 처리량을 증가시킬 수 있습니다.
   * 메모리 관리: **vDNN(가상화된 심층 신경망)** 은 GPU 메모리의 한계를 극복하기 위해 사용됩니다. 학습 중 사용되지 않는 레이어 데이터를 CPU로 오프로드하고, 필요할 때 다시 GPU로 가져옵니다
6. GPU 가속기의 장단점
   * 장점:
     * 높은 프로그래밍 가능성: GPU는 CUDA, cuDNN, TensorRT와 같은 소프트웨어 툴을 통해 매우 유연하게 다양한 응용 프로그램에 적용할 수 있습니다.
     * AI 응용 지원: GPU는 클라우드 및 엣지 디바이스에서 딥러닝 모델을 학습 및 추론할 수 있어, 다양한 AI 응용 분야에서 사용됩니다.
     * 병렬 처리 능력: GPU는 대규모 병렬 작업을 효율적으로 처리하므로 딥러닝에 매우 적합합니다
   * 단점:
     * 전력 효율성: GPU는 TPU(Tensor Processing Unit)와 같은 전용 AI 가속기보다 전력 효율성이 낮습니다.
     * 복잡성: GPU는 복잡한 제어 로직을 가지고 있어 특정 작업에서 AI 전용 하드웨어보다 비효율적일 수 있습니다.
     * 비용 및 크기: GPU는 다른 가속기보다 크고 비쌀 수 있습니다.

7. 요약
   이 장에서는 GPU가 딥러닝에서 어떻게 성능을 극대화할 수 있는지, 그리고 이를 위한 하드웨어 및 소프트웨어 최적화 기술에 대해 설명합니다. GPU의 병렬 처리 능력과 소프트웨어 최적화 툴을 통해 딥러닝 모델을 가속화하는 방법을 다루며, GPU의 장점과 한계도 논의하고 있습니다