# 논문_network_pruning

**네트워크 프루닝(Network Pruning)** 에 대한 내용을 다루고 있으며, 딥러닝 모델의 크기를 줄이고 성능을 유지하면서 효율성을 극대화하는 방법론을 제시하고 있습니다. 간략한 요약은 다음과 같습니다:

1. Pruning의 배경
   * 폰 노이만 구조와 CPU와 메모리의 분리에서 발생하는 에너지 소모 문제를 해결하기 위해 **Pruning(가지치기)** 가 필요합니다. 특히 딥러닝 모델에서는 계산 및 메모리 접근에 많은 에너지가 소모되며, 이를 최적화하는 것이 중요합니다.
2. Pruning의 개념 및 방법
   * Pruning은 신경망의 가중치 일부를 제거해 파라미터 수와 계산량을 줄이는 기법으로, 불필요한 연결을 제거하고 성능을 유지합니다.
   * 이 방법은 주로 모델의 학습이 완료된 후 가지치기를 적용하고 **미세 조정(fine-tuning)** 을 통해 성능을 회복하는 과정으로 이루어집니다.
3. Liu(2017)의 실험
   * Liu의 연구에서는 가지치기 후 재학습하는 방법 대신 가중치를 초기화한 후 학습해도 동일한 성능을 유지할 수 있다는 결과를 제시합니다.
   * 이는 가지치기된 대규모 모델이 작은 네트워크 아키텍처로 대체될 수 있음을 의미하며, Pruning 후의 모델에서 학습된 가중치의 유용성이 낮을 수 있음을 시사합니다.
4. Network Pruning as Architecture Search
   * Pruning은 단순히 모델 크기를 줄이는 것이 아니라, 더 나아가 효율적인 아키텍처 탐색 방법으로도 사용할 수 있습니다. Pruning을 통해 네트워크 구조를 재구성함으로써 더 나은 성능을 낼 수 있습니다.
   * 이 과정에서 다양한 구조적 가지치기 기법이 사용되며, 가지치기를 통한 네트워크 구조의 변화를 탐구합니다.
5. 결론
   * Pruning은 단순히 모델의 파라미터를 줄이는 것 이상으로, 효율적인 네트워크 구조를 발견하는 데 중요한 역할을 할 수 있습니다.
   * 연구 결과, 구조적 가지치기가 무작위 초기화 모델과 유사한 성능을 낼 수 있으며, 더 효율적인 학습이 가능하다는 점을 강조합니다