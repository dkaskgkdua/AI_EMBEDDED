# 성능 향상법(1)

1. Introduction
   딥러닝 모델의 성능을 개선하기 위해 **깊은 파이프라이닝(deeper pipelining)** 이 일반적으로 사용되지만, 이는 클럭 주파수를 높이기 위해 모듈을 세분화함으로써 전력 소비가 증가합니다.
   데이터 이동량이 증가하면 칩 내에서 에너지 소비가 증가하며, 외부 메모리 접근 시 더 큰 지연 시간과 에너지 소모가 발생하게 됩니다. CPU 및 GPU는 이러한 문제에 대응하지 않고, 단순히 8비트 또는 16비트 데이터 폭의 양자화(quantization)를 적용해 성능을 향상시키려 합니다

2. Model Compression
   모델 압축은 네트워크의 복잡도를 줄이고, 효율성을 높이기 위한 방법입니다.<br>주요 기법으로는 Pruning(가지치기), Dropout & DropConnect, Distillation(지식 증류), **Weight-sharing(가중치 공유)**가 있습니다.

3. Pruning
   * **Pruning(가지치기)** 는 신경망이 불필요한 유닛이나 연결을 가지고 있을 때, 그 영향을 최소화하여 모델의 크기를 줄이는 기법입니다.
   * Pruning의 개념: 가중치가 특정 임계값보다 낮으면 해당 연결을 잘라냅니다. 모든 연결이 잘린 유닛은 제거됩니다.
   * Pruning의 유형:
     1) 비구조적 Pruning: 학습 중에 무작위로 0 값이 될 요소를 제거.
     2) 구조적 Pruning: 특정 위치의 요소들을 그룹화하여 모델 압축을 쉽게 만듭니다.
   * Pruning의 세부 유형:
     1) Fine-grained Pruning: 데이터 구조와 관계없이 네트워크 파라미터를 독립적으로 제거.
     2) Vector-level/kernel-level Pruning: 1D 벡터와 2D 커널에서 파라미터를 제거하여 구조화된 희소성을 만듦.
     3) Group-level Pruning: 동일한 희소 패턴을 따르는 필터의 파라미터를 제거.
     4) Filter-level Pruning: 필터 크기를 줄여서 네트워크 아키텍처를 단순화함.
   * Pruning의 실험 결과:
     1) LeNet, AlexNet, VGGNet에서 10배 압축 성능을 확인.
     2) AlexNet의 경우, Conv 레이어가 Fully Connected 레이어보다 가지치기에 더 민감함

4. Dropout & DropConnect
   * Dropout: 과적합을 방지하기 위한 간단한 기법으로, 학습 중에 MLP 네트워크의 활성화 함수 중 일부를 확률적으로 무효화합니다.
   * DropConnect: Dropout과 유사하지만, 활성화 함수 대신 연결(edge)을 무효화합니다. DropConnect는 fully connected 레이어에서 효과적입니다.
   * Pruning과 Dropout/DropConnect의 차이:
     * Dropout/DropConnect는 학습 중 확률적으로 연결을 제거하여 모델의 표현력을 향상시키고, 추론 단계에서 유닛이나 파라미터를 줄이지 않음.
     * Pruning은 추론 단계에서도 모델의 복잡성을 줄이고, 에너지 효율성을 향상시킵니다.

5. Distillation
   * **Distillation(지식 증류)** 는 큰 신경망에서 학습된 지식을 작은 신경망에 증류하여 작고 효율적인 모델을 만드는 기법입니다.
   * Distillation은 Softmax 함수의 온도 매개변수 𝑇를 조절하여 출력을 부드럽게 만들고, 이를 통해 숨겨진 지식을 학습합니다.
   * Distillation Loss: 학생 모델은 교사 모델의 예측 값을 학습하며, 이를 Soft-target loss라고 합니다. 반면 라벨을 사용한 손실을 Hard-target loss라고 부릅니다.
   * Distillation의 효과: ResNet과 같은 모델에서 distillation을 적용하여 성능을 유지하면서도 더 작은 모델로 변환할 수 있습니다.

6. Weight-sharing
   * Weight-sharing은 한 층 또는 여러 층에서 가중치를 공유하여, 전체 가중치의 수를 줄이는 기법입니다.
   * Weight-sharing의 예시: 동일한 범위 내의 가중치를 그룹으로 묶어 동일한 인덱스를 사용하여 가중치를 공유하고, 이 그룹화된 가중치의 합계를 통해 업데이트를 진행합니다

7. Numerical Compression
   * 수치 압축은 데이터 폭 감소, 즉 가중치의 표현 범위를 줄여 계산을 단순화하는 기법입니다. 이 기법은 모델의 정확도에 큰 영향을 주지 않으면서도 연산 복잡도를 줄일 수 있습니다

8. Encoding
   * Encoding은 데이터를 효율적으로 저장하고 전송하기 위한 방법으로, pruning과 결합하여 희소성을 제거하고 메모리 사용량을 줄입니다

9. Zero-skipping
   * Zero-skipping은 희소 모델에서 0 값 요소를 건너뛰어 불필요한 연산을 피하는 기법입니다. 이를 통해 외부 메모리 접근 횟수를 줄이고 전력 소비를 절감할 수 있습니다.

10. Approximation
    * 근사 연산은 신경망 계산에서 정확도를 약간 희생하더라도 연산 복잡도를 줄이는 방법입니다. 이를 통해 성능 향상과 에너지 절약을 동시에 달성할 수 있습니다

11. Optimization
    * 최적화 기법은 하드웨어 및 소프트웨어 차원에서 성능을 극대화하기 위한 다양한 방법들을 포함합니다. 예를 들어, 메모리 관리나 데이터 흐름 최적화 등을 통해 하드웨어의 효율성을 높일 수 있습니다

12. Summary
    * 본 챕터에서는 신경망 모델의 성능을 향상시키기 위한 여러 기법들을 소개하였습니다. 모델 압축, 수치 압축, encoding, zero-skipping, 근사 연산, 최적화 등의 기법들은 신경망 모델의 연산 복잡도를 줄이고, 하드웨어 효율성을 극대화하기 위해 사용됩니다