# 성능 향상법(3)
신경망 하드웨어의 성능을 향상시키기 위한 다양한 방법을 소개하며, 데이터 압축, 인코딩, 제로 스키핑, 근사화 및 최적화 기법을 다룹니다. 아래는 각 항목을 목차별로 누락 없이 자세히 설명한 내용입니다.

1. Encoding
   * 인코딩은 신경망의 파라미터나 활성화 데이터를 압축하는 방법입니다. 여기에는 런 길이 인코딩(RLC)과 허프만 인코딩(Huffman Coding)이 포함됩니다.
   * Run-length encoding은 연속된 동일 데이터를 하나의 데이터와 길이 정보로 표현하여 메모리 사용을 줄입니다.
     * 예시로 Eyeriss에서는 RLC를 사용해 메모리 대역폭을 절약하며, 0이 많은 신경망 데이터에서 매우 효과적입니다.
   * Huffman Coding은 데이터의 등장 빈도를 기반으로 가변 길이 코드를 생성해 메모리 요구량을 줄입니다.
     * 허프만 코딩은 특히 작은 파라미터 세트에 효과적이며, 3-4 비트의 비트폭을 줄일 수 있습니다
2. Zero-Skipping
   * 제로 스키핑은 신경망에서 0값을 건너뛰는 기법으로, 연산 효율성을 높입니다. 이는 특히 ReLU 함수가 많은 0값을 생성하기 때문에 적용됩니다.
   * 보고된 결과에 따르면, 완전 연결 층에서는 약 40%의 파라미터가 0이므로, 이러한 값을 건너뛰면 에너지를 절약하고 지연 시간을 줄일 수 있습니다.
   * CSR(Compressed Sparse Row) 및 CSC(Compressed Sparse Column) 인코딩을 사용해 희소 텐서를 효율적으로 표현하고, 메모리 접근을 줄이는 방식도 포함됩니다
3. Approximation (근사화)
   * 근사화는 신경망의 내재된 강건성을 활용해, 정확성을 약간 희생하면서도 계산 복잡도를 줄이는 방법입니다. 이는 활성화 함수, 곱셈기 등의 연산을 간단하게 만들어 하드웨어에서 에너지를 절약하고 성능을 높입니다.
   * 활성화 함수 근사화: 주요 활성화 함수를 간단한 클리핑 함수로 근사화하여, 예를 들어 ReLU6와 같은 방식으로 최대값을 6으로 제한합니다.
   * 곱셈기 근사화:
     * 시프트 연산을 사용하여 곱셈을 근사화하거나, **Lookup Table (LUT)**을 사용해 작은 값 범위에서의 곱셈을 효율적으로 처리할 수 있습니다
4. Model Optimization
   * 모델 최적화는 신경망의 파라미터 수를 줄이거나, 메모리 접근을 최소화하여 성능을 높이는 방법입니다.
   * Combined Optimization: SqueezeNet과 같은 모델은 50-510배 적은 파라미터를 사용하면서도 AlexNet과 유사한 정확도를 달성하여, 엣지 장치에서 실행할 수 있도록 최적화됩니다.
   * Fused Layer Optimization: 여러 계층을 하나로 결합하여 외부 메모리로의 접근을 최소화하고, 데이터 재사용을 극대화할 수 있습니다. 이를 통해 DRAM 전송량을 줄이고, 칩 내에서 데이터 흐름을 최적화할 수 있습니다
5. Data-flow Optimization
   * 신경망 모델의 데이터 흐름을 최적화하여 칩 내 메모리 사용을 효율화하고, 외부 메모리 접근을 줄이는 방법입니다.
   * Tiling 기법을 사용해 매트릭스 곱셈 연산에서 데이터 재사용을 극대화합니다. 이 기법은 반복된 데이터 전송을 줄여 메모리 대역폭을 절약합니다.
   * 세 가지 데이터 재사용 유형:
     1) Convolutional Reuse: 동일한 입력 특징 맵과 필터 가중치가 여러 번 사용됨.
     2) Feature Map Reuse: 하나의 특징 맵에 여러 필터가 적용됨.
     3) Filter Reuse: 동일한 필터 가중치가 여러 입력 특징 맵에 사용됨
6. Summary
   * 이 장에서는 신경망의 성능을 향상시키기 위한 다양한 방법들을 종합적으로 설명하였습니다. 인코딩, 제로 스키핑, 근사화, 모델 최적화, 데이터 흐름 최적화 등의 기법을 통해 메모리 사용량을 줄이고, 계산 효율성을 높이며, 에너지 소비를 절감하는 방법을 소개했습니다.